{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Predicting Protein Function from Sequence Using ESM-2 Embeddings\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project demonstrates how to predict protein molecular functions directly from amino acid sequences using transfer learning with ESM-2, a state-of-the-art protein language model. We build a multi-label classifier that assigns Gene Ontology (GO) functional annotations to human proteins based on learned sequence representations.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "**Traditional approach:**\n",
    "- Experimental characterization of protein function is slow and expensive\n",
    "- Homology-based methods (BLAST) fail for proteins without known relatives\n",
    "- Functional annotation gap: millions of sequenced proteins remain uncharacterized\n",
    "\n",
    "**Our approach:**\n",
    "- Leverage ESM-2's pre-trained knowledge of evolutionary patterns\n",
    "- Treat sequences as \"text\" and learn function from context\n",
    "- Enable rapid functional annotation at scale\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### 1. Data Collection and Preparation\n",
    "\n",
    "**Sources:**\n",
    "- **GO Annotations:** Gene Ontology Consortium human annotation file (`goa_human.gaf`)\n",
    "  - 93,568 protein-function associations\n",
    "  - 18,447 unique human proteins\n",
    "  - 4,776 distinct GO molecular function terms\n",
    "\n",
    "- **Protein Sequences:** UniProt human proteome FASTA\n",
    "  - Mapped sequences to GO annotations via UniProt accession IDs\n",
    "  - Filtered for sequence length ≤500 amino acids (computational efficiency)\n",
    "\n",
    "**Data Processing:**\n",
    "1. Removed uninformative root-level GO terms (e.g., \"binding\", \"molecular_function\")\n",
    "2. Filtered for functions with ≥50 training examples (statistical robustness)\n",
    "3. Final dataset: **8,704 proteins × 202 GO terms**\n",
    "4. Multi-label format: Each protein can have multiple functions (average: 3-6 per protein)\n",
    "\n",
    "**Dataset Split:**\n",
    "- Training: 5,222 proteins (60%)\n",
    "- Validation: 1,741 proteins (20%)\n",
    "- Test: 1,741 proteins (20%)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Feature Extraction with ESM-2\n",
    "\n",
    "**Model Selection:**\n",
    "We use **ESM-2 t30 150M** (`facebook/esm2_t30_150M_UR50D`) as a frozen feature extractor:\n",
    "- 30 transformer layers\n",
    "- 150 million parameters\n",
    "- 640-dimensional embeddings per amino acid\n",
    "- Trained on 250M protein sequences via masked language modeling\n",
    "\n",
    "**Why ESM-2?**\n",
    "- Captures evolutionary constraints and structural propensities from sequence alone\n",
    "- No need for 3D structures or homology information\n",
    "- Learns biochemical properties (hydrophobicity, charge, secondary structure propensity) without explicit supervision\n",
    "\n",
    "**Embedding Extraction:**\n",
    "1. Tokenize amino acid sequences\n",
    "2. Forward pass through ESM-2 (no gradient computation)\n",
    "3. Mean-pool across sequence length: (batch, seq_len, 640) → (batch, 640)\n",
    "4. Result: Single 640-dimensional vector per protein\n",
    "\n",
    "**Computational Requirements:**\n",
    "- Processing 8,704 proteins took ~2-3 hours on CPU\n",
    "- Batch size: 16 proteins\n",
    "- Embeddings saved to disk for reproducibility\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Quick Validation: Cellular Localization Test\n",
    "\n",
    "Before processing the full dataset, we validated that ESM-2 embeddings capture biological signal:\n",
    "\n",
    "**Experiment:**\n",
    "- Sample: 20 extracellular + 20 membrane proteins\n",
    "- Model: Small ESM-2 (8M parameters) for speed\n",
    "- Visualization: t-SNE dimensionality reduction (640D → 2D)\n",
    "\n",
    "**Result:**\n",
    "Clear separation between extracellular and membrane proteins in embedding space, confirming ESM-2 learned location-relevant sequence features without explicit training.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Classifier Architecture\n",
    "\n",
    "**Model:** Feed-forward neural network with batch normalization\n",
    "```\n",
    "Input (640D embeddings)\n",
    "    ↓\n",
    "Linear(640 → 512) + BatchNorm + ReLU + Dropout(0.3)\n",
    "    ↓\n",
    "Linear(512 → 256) + BatchNorm + ReLU + Dropout(0.3)\n",
    "    ↓\n",
    "Linear(256 → 202) + Sigmoid\n",
    "    ↓\n",
    "Output (202 GO term probabilities)\n",
    "```\n",
    "\n",
    "**Design Choices:**\n",
    "- **Sigmoid activation:** Multi-label classification (each function predicted independently)\n",
    "- **Batch Normalization:** Stabilizes training, reduces overfitting\n",
    "- **Dropout (0.3):** Regularization to prevent memorization\n",
    "- **Loss:** Binary Cross-Entropy (BCE) for multi-label targets\n",
    "- **Optimizer:** Adam (lr=0.001)\n",
    "- **Early Stopping:** Patience=5 epochs based on validation loss\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Hyperparameter Experimentation\n",
    "\n",
    "We systematically compared 5 configurations:\n",
    "\n",
    "| Configuration | Architecture | Dropout | LR | Batch Norm | Val Loss |\n",
    "|---------------|--------------|---------|-------|------------|----------|\n",
    "| **batch_norm** | [512, 256] | 0.3 | 0.001 | ✓ | **0.0743** |\n",
    "| baseline | [512, 256] | 0.3 | 0.001 | ✗ | 0.0871 |\n",
    "| higher_dropout | [512, 256] | 0.5 | 0.001 | ✗ | 0.0876 |\n",
    "| deeper | [512, 256, 128] | 0.3 | 0.001 | ✗ | 0.1091 |\n",
    "| lower_lr | [512, 256] | 0.3 | 0.0001 | ✗ | 0.3683 |\n",
    "\n",
    "**Winner:** Batch normalization configuration (best validation loss, most stable training)\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "### Overall Performance\n",
    "\n",
    "**Test Set Metrics (Best Model):**\n",
    "- **Loss:** 0.0739\n",
    "- **Accuracy:** 98.57%\n",
    "- **Mean PR-AUC:** 0.123 (averaged over 200/202 GO terms)\n",
    "\n",
    "**Baseline Comparison:**\n",
    "\n",
    "| Method | Mean PR-AUC | Improvement |\n",
    "|--------|-------------|-------------|\n",
    "| Uniform Random | 0.016 | - |\n",
    "| Proportional Random | 0.016 | - |\n",
    "| **ESM-2 Model** | **0.123** | **~8x better** |\n",
    "\n",
    "The model significantly outperforms random baselines, demonstrating that ESM-2 embeddings capture biologically meaningful sequence-function relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### Per-Function Analysis\n",
    "\n",
    "**Top Performing Functions (PR-AUC >0.85):**\n",
    "1. **Olfactory receptor activity** (0.996)\n",
    "   - 7 transmembrane domains with conserved sequence motifs\n",
    "   - ESM-2 easily detects this repetitive structural pattern\n",
    "\n",
    "2. **G protein-coupled receptor activity** (0.991)\n",
    "   - Highly conserved topology across GPCR family\n",
    "   - Strong sequence signature\n",
    "\n",
    "3. **Protein kinase activity** (0.892)\n",
    "   - Conserved ATP-binding domain and catalytic loop\n",
    "   - Large training set (101 examples)\n",
    "\n",
    "4. **DNA-binding transcription factors** (0.903)\n",
    "   - Zinc fingers, helix-turn-helix motifs\n",
    "   - Charged residues (K, R) for DNA interaction\n",
    "\n",
    "**Challenging Functions (PR-AUC <0.1):**\n",
    "- DNA/RNA helicase activity\n",
    "- Microtubule motor activity\n",
    "- Cytokine receptor activity\n",
    "\n",
    "**Key Insight:** Performance strongly correlates with training data availability (r² ~0.6). Functions with <20 examples struggle (PR-AUC <0.1), while those with >50 examples achieve robust predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Dynamics\n",
    "\n",
    "**Observations:**\n",
    "- Early stopping at epoch ~40-45 (validation loss plateaus)\n",
    "- No overfitting: train and validation losses track closely\n",
    "- Batch normalization enables stable convergence over 50 epochs\n",
    "- Gap between train/val loss: ~0.01 (healthy generalization)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Transfer learning works for proteins:** ESM-2 embeddings enable functional prediction without task-specific pre-training\n",
    "\n",
    "2. **Data quantity matters:** Functions with ≥50 training examples achieve PR-AUC >0.5, while rare functions (<20) remain challenging\n",
    "\n",
    "3. **Conserved motifs = better predictions:** Functions with characteristic sequence signatures (kinases, receptors, DNA-binding domains) perform best\n",
    "\n",
    "4. **Multi-label complexity:** Proteins have 3-6 functions on average, requiring independent probability estimates per function\n",
    "\n",
    "5. **Computational efficiency:** Once embeddings are extracted, classifier trains in <5 minutes on GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations and Future Work\n",
    "\n",
    "**Current Limitations:**\n",
    "- **Rare function problem:** 90% of GO terms have <50 examples\n",
    "- **Class imbalance:** Common functions (metal binding) dominate training signal\n",
    "- **Sequence length restriction:** Excluded proteins >500 residues (15% of proteome)\n",
    "- **Single ontology:** Only molecular function (MFO), not biological process or cellular component\n",
    "\n",
    "**Future Directions:**\n",
    "1. **Hierarchical loss functions:** Exploit GO term parent-child relationships\n",
    "2. **Few-shot learning:** Improve predictions for rare functions using meta-learning\n",
    "3. **Larger ESM-2 models:** Test 650M or 3B parameter versions for quality gains\n",
    "4. **Multi-task learning:** Jointly predict MFO + BPO + CCO ontologies\n",
    "5. **Attention visualization:** Identify which residues drive each function prediction\n",
    "6. **Active learning:** Prioritize experimental validation for high-uncertainty predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Stack\n",
    "\n",
    "**Core Libraries:**\n",
    "- `transformers`: ESM-2 model loading and inference\n",
    "- `torch`: Neural network implementation and training\n",
    "- `scikit-learn`: Train/test splits, metrics (ROC-AUC, PR-AUC)\n",
    "- `biopython`: FASTA parsing and sequence handling\n",
    "- `pandas`: Data manipulation\n",
    "- `matplotlib/seaborn`: Visualization\n",
    "\n",
    "**Data Sources:**\n",
    "- Gene Ontology Consortium (GO annotations)\n",
    "- UniProt (protein sequences)\n",
    "- ESM-2 pre-trained weights (Meta AI / Facebook Research)\n",
    "\n",
    "---\n",
    "\n",
    "## Reproducibility\n",
    "\n",
    "All code, data processing steps, and model configurations are documented in this notebook. Key decisions:\n",
    "- Random seed: 42 (train/test splits)\n",
    "- ESM-2 checkpoint: `facebook/esm2_t30_150M_UR50D`\n",
    "- Early stopping patience: 5 epochs\n",
    "- Minimum GO term frequency: 50 examples\n",
    "\n",
    "Embeddings saved to disk allow re-training classifiers without re-computing ESM-2 forward passes.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates that protein language models like ESM-2 enable accurate functional annotation from sequence alone, bypassing the need for homology search or experimental characterization. The approach scales to proteome-wide analysis and provides interpretable predictions via GO term descriptions.\n",
    "\n",
    "**Main Contribution:** End-to-end pipeline from raw sequences → functional predictions, with systematic evaluation of model architectures and biological interpretation of results.\n",
    "\n",
    "The 8x improvement over random baselines and strong performance on well-characterized protein families (GPCRs, kinases) validates the biological relevance of learned representations. This methodology can accelerate functional annotation for newly sequenced organisms and guide experimental prioritization.\n",
    "- Insight: Performance strongly correlates with training frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Select device: MPS for Apple Silicon, else CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10000, 10000)\n",
    "x = x.to(device)\n",
    "y = torch.matmul(x, x)\n",
    "print(\"OK, GPU working.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Load ESM-2 Protein Language Model\n",
    "\n",
    "ESM-2 (Evolutionary Scale Modeling v2) is a transformer-based protein language model trained on 250M protein sequences from UniRef50. It learns biochemical and evolutionary patterns directly from amino acid sequences without explicit supervision.\n",
    "\n",
    "**Available ESM-2 Variants:**\n",
    "\n",
    "| Model | Layers | Parameters | Embedding Dim | Speed | Quality |\n",
    "|-------|--------|------------|---------------|-------|---------|\n",
    "| esm2_t6_8M | 6 | 8M | 320 | Fastest | Basic |\n",
    "| esm2_t12_35M | 12 | 35M | 480 | Fast | Good |\n",
    "| esm2_t30_150M | 30 | 150M | 640 | Moderate | Very Good |\n",
    "| esm2_t33_650M | 33 | 650M | 1280 | Slow | Excellent |\n",
    "| esm2_t36_3B | 36 | 3B | 2560 | Very Slow | State-of-art |\n",
    "| esm2_t48_15B | 48 | 15B | 5120 | Extremely Slow | Best |\n",
    "\n",
    "**Selected Model: esm2_t30_150M_UR50D**\n",
    "- **Why:** Optimal balance between quality and computational cost\n",
    "- 640-dimensional embeddings capture sufficient biological detail\n",
    "- Processes 8,704 proteins in ~2-3 hours on CPU\n",
    "- Standard choice in published research (comparable to ProteinBERT, ESM-1b)\n",
    "- Larger models (650M+) offer marginal gains but 3-10x slower\n",
    "\n",
    "**Training Method:** Masked language modeling - predict hidden amino acids from surrounding context, forcing the model to learn evolutionary constraints and structural dependencies.\n",
    "\n",
    "I used this pre-trained model as a frozen feature extractor, leveraging knowledge from 250M sequences without retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "### SETUP: Load ESM-2 Protein Language Model\n",
    "\n",
    "ESM-2 (Evolutionary Scale Modeling v2) is a transformer-based protein language model trained on 250M protein sequences from UniRef50. It learns biochemical and evolutionary patterns directly from amino acid sequences without explicit supervision.\n",
    "\n",
    "**Model: esm2_t30_150M_UR50D**\n",
    "- 30 transformer layers\n",
    "- 150M parameters\n",
    "- 640-dimensional embeddings per amino acid\n",
    "- Trained via masked language modeling (predict hidden amino acids from context)\n",
    "\n",
    "We use this pre-trained model as a frozen feature extractor to generate embeddings that capture protein structure and function without training from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358,
     "referenced_widgets_removed": [
      "878e7ce96c9147a585efaf5c373e594f",
      "88fab252a54049f09898d649a944edf2",
      "2139ad4deae44a8ba5b501c0dac767e2",
      "a5d2eda57cb847c5846f681eda2807ee",
      "6b68f67dffbe49afa9d81e444a0cee53",
      "8ec9556c74d645d599b869808d8d0741",
      "ee9b37990a004f6d84b25b0feedb2ae4",
      "ba3b24fe30b445a9b89d8d8ef989b920",
      "05302d19301541d6bf2c49d28801b4a3",
      "a3de90e7401243c790d506804b9e94f6",
      "00b102a0c30043e78dfa05cce8d65a8b",
      "5c505c513a2a4c57a086cd2ad398d58e",
      "06c80ebcba0b4a43abbeebde7e1c4260",
      "4eab278d2832468a9f6b4f3e2ce02bae",
      "c0f6bb818188408997a09cfe7d16156b",
      "65b40b0ce6014ebabf28cd3038c44bba",
      "83ae56e0695442c4b9ca42883dab046a",
      "321e75ada30c40289fdfdcb3556db786",
      "c230e54fd4b647aa82f222fc274c54a9",
      "c39f355f8d8349ccadd90673afdce12b",
      "91b2d0e1ed074f08824b61ba0a5570be",
      "30c65e47878a4c6ca8c04ba8fcf2746a",
      "bce7f6334b7449c1bd9f5c4267d42355",
      "60c2751267c54687847626af45af08c3",
      "ac264d2fffcd4bbbb176023d15778ed3",
      "a62ed6a656f24b6ba10755d331950344",
      "d76301463cc843c7b5aa37a18c011b6d",
      "1befc64a54b446a4a78df8279f353e54",
      "67e065ef9af94fd79d66d8dfcaef5abe",
      "d957368f224c4155b1e0f8484b6381e8",
      "8eb6f53e8d764b619cda0e144f94585a",
      "17c9cceadfa3462f89c0f3a863cf1212",
      "476792e4a59a41739f41aca2663e46c1",
      "8cc74b889508420ab9d57836407e13c4",
      "7ec9e75c75ec4862867f1f6b5acda42a",
      "0e4e4f4c3d0040818b24c37e8fb8bcf9",
      "ddb1e4924ecd4424b2b225280f5e7dc0",
      "e90cc44b2b6f4bab905b296bac47b724",
      "17bcbad2a99a4bbe87d8ddc7a5e8cfd5",
      "328507fceb3d4444957181dad9361767",
      "dca578d454a24c52a10254eecf2c97ad",
      "b0778ec96f7c464caaf1b20eb49a4949",
      "a7ce8171cabf4d0183a8566eee3f12a0",
      "2cfb56abbbb345aeb78f5f5370b15f54",
      "3129d051a880491ca6868945e9aeb28e",
      "681f2db312c0445bb2cf2a3560fbe6e5",
      "3a7dfa954ec64e089c36f8246d531104",
      "362157ed121741ef87eb4c6b1e64038a",
      "480e01534efb40a8bac152adc8e1dfc9",
      "bc4450641dac414299b4d32b4504f15b",
      "33e81725d72147c592c4fae29b1cb11b",
      "473863f49194451a826c9b0a6fe5efbf",
      "9fcf7d249b744e19958e7bed11ce70a9",
      "0609caaee5d242ab88b3d3721e2ef99f",
      "8d9a0b21681e46a59a87857545059b18"
     ]
    },
    "id": "gAS4scHXb_x_",
    "outputId": "8f50b5a5-50c6-4492-be04-9216dd341768"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "\n",
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = EsmModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(\"ESM2 (650M) loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "32f8d907"
   },
   "source": [
    "# Learning the Language of Proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "f4378503"
   },
   "source": [
    "## Representations of Proteins and Protein LMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Protein Structure Visualization\n",
    "\n",
    "This section demonstrates how to fetch and visualize 3D protein structures from the Protein Data Bank (PDB). While not required for our classification pipeline, it helps understand what proteins look like structurally.\n",
    "\n",
    "**Note:** We work with amino acid sequences (1D), not 3D structures, for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "802440fa",
    "outputId": "fd7779f4-83fb-4a72-95cd-2e98eeaf0230",
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def fetch_protein_structure(pdb_id: str) -> str:\n",
    "  \"\"\"Grab a PDB protein structure from the RCSB Protein Data Bank.\"\"\"\n",
    "  url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "  response = requests.get(url)\n",
    "  return response.text\n",
    "\n",
    "\n",
    "# The Protein Data Bank (PDB) is the main database of protein structures.\n",
    "# Each structure has a unique 4-character PDB ID. Below are a few examples.\n",
    "protein_to_pdb = {\n",
    "  \"insulin\": \"3I40\",  # Human insulin – regulates glucose uptake.\n",
    "  \"collagen\": \"1BKV\",  # Human collagen – provides structural support.\n",
    "  \"proteasome\": \"1YAR\",  # Archaebacterial proteasome – degrades proteins.\n",
    "}\n",
    "\n",
    "protein = \"proteasome\"  # @param [\"insulin\", \"collagen\", \"proteasome\"]\n",
    "pdb_structure = fetch_protein_structure(pdb_id=protein_to_pdb[protein])\n",
    "\n",
    "pdbview = py3Dmol.view(width=400, height=300)\n",
    "pdbview.addModel(pdb_structure, \"pdb\")\n",
    "pdbview.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "pdbview.zoomTo()\n",
    "pdbview.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "0d33333e"
   },
   "source": [
    "## Numerical Representation of a Protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7905d130",
    "outputId": "53261b1c-54e6-4b17-f006-59ad6665b244"
   },
   "outputs": [],
   "source": [
    "# Precursor insulin protein sequence (processed into two protein chains).\n",
    "insulin_sequence = (\n",
    "  \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n",
    "  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n",
    ")\n",
    "print(f\"Length of the insulin protein precursor: {len(insulin_sequence)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "50943897"
   },
   "source": [
    "## One-Hot Encoding of a Protein Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a64df222",
    "outputId": "78086f97-a716-4f0c-95b1-3e49019f0673"
   },
   "outputs": [],
   "source": [
    "amino_acids = [\n",
    "  \"R\", \"H\", \"K\", \"D\", \"E\", \"S\", \"T\", \"N\", \"Q\", \"G\", \"P\", \"C\", \"A\", \"V\", \"I\",\n",
    "  \"L\", \"M\", \"F\", \"Y\", \"W\",\n",
    "]\n",
    "\n",
    "amino_acid_to_index = {\n",
    "  amino_acid: index for index, amino_acid in enumerate(amino_acids)\n",
    "}\n",
    "\n",
    "# Display first 5 items\n",
    "print(dict(list(amino_acid_to_index.items())[:5]))\n",
    "print(f\"... ({len(amino_acid_to_index)} total amino acids)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81b9c6ac",
    "outputId": "16e011c3-0866-4338-9a9c-710940c5f0bb"
   },
   "outputs": [],
   "source": [
    "# Methionine, alanine, leucine, tryptophan, methionine.\n",
    "tiny_protein = [\"M\", \"A\", \"L\", \"W\", \"M\"]\n",
    "\n",
    "tiny_protein_indices = [\n",
    "  amino_acid_to_index[amino_acid] for amino_acid in tiny_protein\n",
    "]\n",
    "\n",
    "tiny_protein_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa3e337d",
    "outputId": "544635c5-5b46-44e2-ba10-a16922295772"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "one_hot_encoded_sequence = torch.nn.functional.one_hot(\n",
    "    torch.tensor(tiny_protein_indices), \n",
    "    num_classes=len(amino_acids)\n",
    ")\n",
    "\n",
    "print(one_hot_encoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "01406aab",
    "mystnb": {
     "figure": {
      "caption": "One-hot encoded representation of a toy protein sequence (`MALWM`), visualized with a heatmap. This binary matrix encodes the identity of each residue without implying any similarity between them",
      "name": "one_hot_matrix_visualized"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    },
    "outputId": "849d6277-22f6-4833-f80d-50c84dc54c4e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig = sns.heatmap(\n",
    "  one_hot_encoded_sequence, square=True, cbar=False, cmap=\"inferno\"\n",
    ")\n",
    "fig.set(xlabel=\"Amino Acid Index\", ylabel=\"Protein Sequence\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## What ESM-2 Learned: Visualizing Amino Acid Embeddings\n",
    "\n",
    "ESM-2's input embeddings capture biochemical properties without explicit supervision. We visualize how the model organizes the 20 amino acids in embedding space using t-SNE dimensionality reduction.\n",
    "\n",
    "**Key Observation:** Amino acids cluster by biochemical properties:\n",
    "- **Hydrophobic** (A, F, I, L, M, V, W, Y): Cluster together\n",
    "- **Charged positive** (H, K, R): Separate group\n",
    "- **Charged negative** (D, E): Separate group  \n",
    "- **Polar uncharged** (N, Q, S, T): Intermediate position\n",
    "- **Special cases** (C, G, P): Distinct due to unique structural roles\n",
    "\n",
    "**Interpretation:** ESM-2 learned amino acid chemistry purely from sequence patterns, never explicitly told about hydrophobicity, charge, or structure. This emergent organization demonstrates the model captures evolutionary and biophysical constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "2FYHVk_mhWw4"
   },
   "source": [
    "I use the ESM2 model trained on UniRef50 because UniRef clustering reduces redundancy in the protein space, enabling the model to learn evolutionary and structural patterns rather than memorizing highly similar sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d785b01c",
    "outputId": "b1423ea8-34d2-4b0e-cb28-869ac347c7bb",
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "\n",
    "# Load the ESM-2 model (33 layers, ~650M parameters) trained on UniRef50.\n",
    "# The model captures evolutionary, structural, and biochemical patterns\n",
    "# directly from protein sequences.\n",
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "# Tokenizer converts amino acid strings into integer token IDs.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Load the pretrained ESM model (not training from scratch).\n",
    "# The output embeddings represent the protein in a high-dimensional space.\n",
    "model = EsmModel.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## ESM-2 Vocabulary\n",
    "\n",
    "The tokenizer uses 33 tokens total:\n",
    "- **20 amino acids** (A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y)\n",
    "- **Special tokens**: `<cls>` (start), `<eos>` (end), `<pad>` (padding), `<unk>` (unknown), `<mask>` (for training)\n",
    "\n",
    "Each token maps to an index used to look up its learned 1280-dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01f9b09b",
    "outputId": "244464c7-b409-4f2e-dd3b-1a031f8eb587"
   },
   "outputs": [],
   "source": [
    "# Obtain the vocabulary dictionary used by the tokenizer\n",
    "# Each key is a token (amino acid or special symbol)\n",
    "# and each value is the corresponding numeric index\n",
    "vocab_to_index = tokenizer.get_vocab()\n",
    "\n",
    "# Print a shortened version of the vocabulary dictionary\n",
    "# (useful to quickly inspect how tokens are encoded)\n",
    "# Example output:\n",
    "# {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, ...}\n",
    "\n",
    "# Print first 10 tokens\n",
    "print(dict(list(vocab_to_index.items())[:10]))\n",
    "print(f\"... ({len(vocab_to_index)} total tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8975e0cd",
    "outputId": "e4db9f12-59af-4940-9c70-b524199f5577"
   },
   "outputs": [],
   "source": [
    "tokenized_tiny_protein = tokenizer(\"MALWM\")[\"input_ids\"]\n",
    "tokenized_tiny_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c45b556",
    "outputId": "730759fa-ae56-4fca-cd37-4f6b9d084db1"
   },
   "outputs": [],
   "source": [
    "tokenized_tiny_protein[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "J6LzPpDujB8y"
   },
   "source": [
    "token_embeddings.shape = (33, 1280) means that the ESM2 model has a vocabulary of 33 tokens (amino acids + special tokens), and each token is represented by a learned 1280-dimensional embedding vector in the input embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Token Embedding Matrix Shape\n",
    "\n",
    "`token_embeddings.shape = (33, 1280)` means:\n",
    "- **33 tokens** in vocabulary (20 amino acids + 13 special tokens)\n",
    "- **1280 dimensions** per token embedding\n",
    "\n",
    "Each of the 33 possible tokens has a learned 1280-dimensional vector that captures its biochemical properties and contextual behavior in protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e5f9653",
    "outputId": "32e13793-ad0f-4fe5-ca04-759e9539354a"
   },
   "outputs": [],
   "source": [
    "#Each token is represented in one vector of 1280 dimensions before getting into the transformer\n",
    "token_embeddings = model.get_input_embeddings().weight.detach().numpy()\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fccb09f4",
    "outputId": "7f83b1d5-ea2a-49ad-80cb-d193c02151eb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(token_embeddings)\n",
    "embeddings_tsne_df = pd.DataFrame(\n",
    "  embeddings_tsne, columns=[\"first_dim\", \"second_dim\"]\n",
    ")\n",
    "embeddings_tsne_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "2BUYElKtjc4j"
   },
   "source": [
    "# Structural organization of the model latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "140cb43f",
    "mystnb": {
     "figure": {
      "caption": "2D t-SNE projection of the learned token embeddings from the ESM2 model. Even without labels, clusters begin to emerge, hinting that the model has learned to organize tokens in a meaningful way.",
      "name": "tsne_no_chemical_properties"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    },
    "outputId": "27cabf3e-054f-4952-c9b1-18a181549f08"
   },
   "outputs": [],
   "source": [
    "fig = sns.scatterplot(\n",
    "  data=embeddings_tsne_df, x=\"first_dim\", y=\"second_dim\", s=50\n",
    ")\n",
    "fig.set_xlabel(\"First Dimension\")\n",
    "fig.set_ylabel(\"Second Dimension\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "vacqSDcfjxtc"
   },
   "source": [
    "We annotate each token according to its biochemical properties to visualize whether the ESM2 embedding space organizes amino acids into meaningful biochemical cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## t-SNE Visualization of Token Embeddings\n",
    "\n",
    "We reduce the 1280-dimensional embeddings to 2D using t-SNE to visualize how ESM-2 organizes amino acids in its learned space.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Biochemical clustering**: Amino acids group by properties (hydrophobic, charged, polar)\n",
    "- **Special tokens separate**: `<cls>`, `<eos>`, `<pad>` cluster away from amino acids\n",
    "- **Emergent organization**: Model learned these relationships purely from sequence data, never explicitly told about biochemistry\n",
    "\n",
    "This demonstrates that ESM-2 embeddings capture meaningful biological properties without supervised labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "edf9d5da",
    "mystnb": {
     "figure": {
      "caption": "Coloring the t-SNE projection by amino acid properties reveals clear clusters of amino acids with similar biochemical roles that tend to group together in embedding space, reflecting the model's ability to capture meaningful biological structure. Technical non-amino acid tokens also group together in this latent space",
      "name": "tsne_with_chemical_properties"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    },
    "outputId": "24db9441-685f-4537-9d35-950fc5f2bb86"
   },
   "outputs": [],
   "source": [
    "# Ensure adjustText is available\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Add token column using vocabulary order\n",
    "embeddings_tsne_df[\"token\"] = list(vocab_to_index.keys())\n",
    "\n",
    "# Biochemically meaningful grouping of tokens\n",
    "token_annotation = {\n",
    "    \"hydrophobic\": [\"A\", \"F\", \"I\", \"L\", \"M\", \"V\", \"W\", \"Y\"],\n",
    "    \"polar uncharged\": [\"N\", \"Q\", \"S\", \"T\"],\n",
    "    \"negatively charged\": [\"D\", \"E\"],\n",
    "    \"positively charged\": [\"H\", \"K\", \"R\"],\n",
    "\n",
    "    # Special biochemical cases:\n",
    "    # C = disulfide bonding\n",
    "    # G = flexibility / no side chain\n",
    "    # P = rigid / helix breaker\n",
    "    \"special case\": [\"C\", \"G\", \"P\"],\n",
    "\n",
    "    # Non-amino-acid tokens used by the ESM tokenizer\n",
    "    \"special token\": [\n",
    "        \"<cls>\", \"<eos>\", \"<mask>\", \"<pad>\", \"<unk>\",\n",
    "        \".\", \"-\", \"<null_1>\", \"<null_2>\", \"<null_3>\", \"<null_4>\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Map every token in the vocabulary to its category\n",
    "embeddings_tsne_df[\"label\"] = embeddings_tsne_df[\"token\"].map(\n",
    "    {token: label for label, tokens in token_annotation.items() for token in tokens}\n",
    ")\n",
    "\n",
    "# Plot embeddings colored by biochemical category\n",
    "fig = sns.scatterplot(\n",
    "    data=embeddings_tsne_df,\n",
    "    x=\"first_dim\",\n",
    "    y=\"second_dim\",\n",
    "    hue=\"label\",\n",
    "    style=\"label\",\n",
    "    s=50,\n",
    ")\n",
    "fig.set_xlabel(\"First Dimension\")\n",
    "fig.set_ylabel(\"Second Dimension\")\n",
    "\n",
    "# Add text labels to each point\n",
    "texts = [\n",
    "    fig.text(row[\"first_dim\"], row[\"second_dim\"], row[\"token\"])\n",
    "    for _, row in embeddings_tsne_df.iterrows()\n",
    "]\n",
    "\n",
    "# Adjust text so labels don't overlap\n",
    "adjust_text(\n",
    "    texts,\n",
    "    expand=(1.5, 1.5),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"grey\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "2a83d250"
   },
   "source": [
    "### The ESM2 Protein Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "SMe792QhmDUK"
   },
   "source": [
    "When we mask one amino acid in the insulin sequence, ESM2 must infer the correct residue using only the surrounding sequence context. This tests whether the language model has learned meaningful biochemical constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "984a71be"
   },
   "outputs": [],
   "source": [
    "# Full amino acid sequence of human insulin (two-chain protein but listed here as a single linear sequence).\n",
    "insulin_sequence = (\n",
    "  \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n",
    "  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n",
    ")\n",
    "\n",
    "# Create a masked version of the sequence to test the model's ability to recover\n",
    "# the correct amino acid from surrounding context.\n",
    "# Here, we replace the amino acid at position 29 (0-based indexing) with <mask>.\n",
    "masked_insulin_sequence = (\n",
    "  \"MALWMRLLPLLALLALWGPDPAAAFVNQH<mask>CGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n",
    "  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n",
    ")\n",
    "\n",
    "# Tokenize the masked sequence: convert each amino acid and special token into an integer ID.\n",
    "masked_inputs = tokenizer(masked_insulin_sequence)[\"input_ids\"]\n",
    "\n",
    "# The tokenizer prepends a <cls> token to the beginning of the sequence.\n",
    "# Therefore, the <mask> token now appears at index (29 + 1) = 30 in the tokenized sequence.\n",
    "assert masked_inputs[30] == vocab_to_index[\"<mask>\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets_removed": [
      "827c689924db4961836450a91d295f0e",
      "cd7650aec1c8449d9e5f156c5eb415ad",
      "1f252330f14c4970bdc7c92f03dd56c2",
      "39ae8ee7e2a140faba05a8a0fc167b0d",
      "d571d173d8534e62b201c636fbfae9c6",
      "be6f7a5b0016452abbcfc561a0c52a31",
      "48a6e91c266245dc8146ca8be1577364",
      "94301c06f7e64b09bbceb52f82a73d3d",
      "2e783271e4944e5987ced71a6f218f49",
      "eb3408adaa864552b6ccaab25c27967d",
      "cfe0c5db5fa647a58edb0b18f6e480cd",
      "ae584a0165444df897f808e47bfebe0e",
      "7a62ec532f084c70af5375227c649499",
      "db66cb4f5b1d4a8aa38ef546644efc97",
      "4d5d007a193240c7884adce1448b33d1",
      "13b617b788e84a2398b7a1cac4546d67",
      "3f3d62b03ab648b590a82e63347a7b9f",
      "a7e0421073704fc39b531721a5ae6c50",
      "93ec41d13c8f45d2954090ed859193f6",
      "fa03334ad5fa405fa802640930e88624",
      "b095946c5ddb46a781a17582169e5ac5",
      "c37220cdb91d4fd3b42c3be1ac82cdf6",
      "99b43ff9df35404e9585d55ac433dc20",
      "297afa0f4753433e8a9a0b0a48797796",
      "e516e963dac8462a8ff49b77ddd94bf8",
      "143c6dad93384565a09da7c4b4ed797f",
      "7342a0164a6244f885d0dc63066954d0",
      "6f31502ce7dd42628fe7ae830430d8dd",
      "bcc7e6acbfa54bf9a6830462a25b8655",
      "75dc2db386b448c5820216d47e9ab85d",
      "f7c5a0a9d6494852995f70fd577af887",
      "7a62c910ecc74958acafedba88cb80f3",
      "0c2ce695cf984ae4811f007b24c50e4d",
      "eda2367f26144a60844e0432daa353c3",
      "40cb29e7dd9d4006b469f51c99bd8319",
      "056ba238b7f5470b91234b80c5d90491",
      "f777d2bd22cf44f28f0b387105b7793c",
      "022274c5469942edb504305c1c9859af",
      "cc1ab496e64e4612bbba7bd09301245c",
      "a6e1c325528b475e8c2a4403d20541d6",
      "b07247a3e24440549b7a914efc2148ca",
      "7770f94fe64845369ab9983fdb1e2fb3",
      "12ae7df3af9c46118a15f7ca530998dc",
      "d488850c464c4fd89ca141f06bfb9011",
      "e3143a01a197463d85d4f7cbf2929f87",
      "1f28ae5a43054977b1f44e115dc7b2a2",
      "2da51dcec2fc4632ad55ffc2283d2b0c",
      "f0178edb951346bf83b8a2cc93aa15f9",
      "bb6511b42480417caf51ef94a9ec53a6",
      "1ce9fbdd996c44b7a844956d5f2e547c",
      "d52699f834044cc59229cc0bf297240c",
      "b72c34567eef443cbc29694179a66fb8",
      "ccb474851662457ea6447a6db926d2d5",
      "6600989b9b7343f6857492e3fadebfa1",
      "afdc6b8884e64930bc54dba3b3e67fec"
     ]
    },
    "id": "f27d3782",
    "outputId": "083c46aa-01d7-4b15-d658-684b1b35b467"
   },
   "outputs": [],
   "source": [
    "from transformers import EsmTokenizer, EsmForMaskedLM\n",
    "\n",
    "# ESM2 model checkpoint\n",
    "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_checkpoint)\n",
    "masked_lm_model = EsmForMaskedLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Masked Amino Acid Prediction\n",
    "\n",
    "ESM-2 was trained using masked language modeling: randomly hide amino acids and predict them from context. This forces the model to learn structural and evolutionary constraints.\n",
    "\n",
    "**Test:** Mask position 30 in insulin sequence and see if ESM-2 can predict the correct amino acid using only surrounding context.\n",
    "\n",
    "**Why this matters:** If the model accurately predicts masked residues, it demonstrates understanding of:\n",
    "- Local sequence patterns (motifs)\n",
    "- Long-range dependencies\n",
    "- Biochemical constraints (e.g., hydrophobic residues in protein core)\n",
    "\n",
    "This capability is why ESM-2 embeddings are powerful features for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "b9dca718",
    "mystnb": {
     "figure": {
      "caption": "Model prediction for a masked leucine (`L`) in the insulin sequence. The model confidently predicts the correct amino acid (`L`) with high probability, showing that it has learned common sequence patterns in proteins",
      "name": "predict_missing_aa"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    },
    "outputId": "31da05d2-e2fc-4105-e82c-1f29e4c6ab7e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "# Load masked LM version (for predictions)\n",
    "masked_lm_model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(masked_insulin_sequence, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "model_outputs = masked_lm_model(**inputs)\n",
    "logits = model_outputs.logits\n",
    "\n",
    "# Position of <mask> (we know it's 30)\n",
    "mask_logits = logits[0, 30]\n",
    "\n",
    "# Softmax → probabilities\n",
    "mask_probs = torch.softmax(mask_logits, dim=0).detach().numpy()\n",
    "\n",
    "# Get vocabulary as ordered list\n",
    "vocab = tokenizer.get_vocab()\n",
    "tokens_sorted = sorted(vocab.items(), key=lambda x: x[1])\n",
    "letters = [t[0] for t in tokens_sorted]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(letters, mask_probs, color=\"grey\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Model Probabilities for the Masked Amino Acid\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fSte0LJm7cJ",
    "outputId": "5686fe88-7c6e-4447-cf28-dbcaa0eec2c1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert logits at the masked position to probabilities\n",
    "mask_probs = torch.softmax(mask_logits, dim=0)\n",
    "\n",
    "# Select the top 5 most probable tokens according to the model prediction\n",
    "top_k = 5\n",
    "topk_probs, topk_indices = torch.topk(mask_probs, k=top_k)\n",
    "\n",
    "# Build inverse vocabulary to map indices back to token strings\n",
    "vocab = tokenizer.get_vocab()\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Convert token indices to amino acid tokens\n",
    "topk_tokens = [inv_vocab[int(idx)] for idx in topk_indices]\n",
    "\n",
    "# Display the top 5 predicted amino acids with their probabilities\n",
    "for token, prob in zip(topk_tokens, topk_probs):\n",
    "    print(f\"{token}: {prob.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "_LRWIiBknJIE"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MaskPredictor: Utility Class for Testing ESM-2's Learned Knowledge\n",
    "# =============================================================================\n",
    "# This class allows us to mask any position in a protein sequence and\n",
    "# visualize what amino acid ESM-2 predicts should be there based on context.\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "from typing import List\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "class MaskPredictor:\n",
    "    \"\"\"Predict masked amino acids using a protein language model.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        \"\"\"\n",
    "        Initialize predictor with tokenizer and model.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Converts sequences to token IDs\n",
    "            model: Pre-trained ESM-2 masked language model\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(\"mps\")  # Move to Apple Silicon GPU\n",
    "\n",
    "    def mask_sequence(self, sequence: str, mask_index: int) -> str:\n",
    "        \"\"\"\n",
    "        Replace amino acid at given position with <mask> token.\n",
    "        \n",
    "        Args:\n",
    "            sequence: Original protein sequence (e.g., \"MALW...\")\n",
    "            mask_index: Position to mask (0-indexed)\n",
    "            \n",
    "        Returns:\n",
    "            Masked sequence (e.g., \"MAL<mask>M...\")\n",
    "        \"\"\"\n",
    "        if mask_index < 0 or mask_index >= len(sequence):\n",
    "            raise ValueError(\"Mask index is outside the sequence length.\")\n",
    "        return sequence[:mask_index] + \"<mask>\" + sequence[mask_index + 1:]\n",
    "\n",
    "    def predict(self, sequence: str, mask_index: int):\n",
    "        \"\"\"\n",
    "        Predict probabilities for all possible amino acids at masked position.\n",
    "        \n",
    "        Args:\n",
    "            sequence: Original protein sequence\n",
    "            mask_index: Position to predict\n",
    "            \n",
    "        Returns:\n",
    "            Array of probabilities (length 33, one per token)\n",
    "        \"\"\"\n",
    "        # Step 1: Create masked version of sequence\n",
    "        masked_sequence = self.mask_sequence(sequence, mask_index)\n",
    "\n",
    "        # Step 2: Tokenize and move to GPU\n",
    "        inputs = self.tokenizer(masked_sequence, return_tensors=\"pt\").to(\"mps\")\n",
    "        \n",
    "        # Step 3: Forward pass through model\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # Step 4: Extract logits for masked position\n",
    "        # +1 because tokenizer adds <cls> token at start\n",
    "        mask_logits = outputs.logits[0, mask_index + 1].cpu()\n",
    "\n",
    "        # Step 5: Convert logits to probabilities via softmax\n",
    "        mask_probs = torch.softmax(mask_logits, dim=0).detach().numpy()\n",
    "        return mask_probs\n",
    "\n",
    "    def plot_predictions(self, sequence: str, mask_index: int) -> Figure:\n",
    "        \"\"\"\n",
    "        Visualize model's predicted probability distribution for masked position.\n",
    "        \n",
    "        Shows bar chart of all 33 tokens with their predicted probabilities.\n",
    "        Highest bar = model's top prediction for what amino acid should be there.\n",
    "        \"\"\"\n",
    "        mask_probs = self.predict(sequence, mask_index)\n",
    "        tokens = list(self.tokenizer.get_vocab().keys())\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.bar(tokens, mask_probs, color=\"grey\")\n",
    "        ax.set_xticklabels(tokens, rotation=90)\n",
    "        ax.set_title(\n",
    "            f\"Predicted probabilities at index {mask_index}\\n\"\n",
    "            f\"(True amino acid = {sequence[mask_index]})\"\n",
    "        )\n",
    "        ax.set_xlabel(\"Tokens\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "579e05c5",
    "mystnb": {
     "figure": {
      "caption": "Model prediction for a masked asparagine (`N`) in the insulin sequence. Here, the model is more uncertain, it assigns moderate probability to several possible amino acids, indicating that this position is harder to predict based on surrounding context",
      "name": "predict_missing_aa_less_clear"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    },
    "outputId": "93ba23c6-b750-4588-f68f-3bf006ff3074"
   },
   "outputs": [],
   "source": [
    "predictor = MaskPredictor(tokenizer, masked_lm_model)\n",
    "\n",
    "predictor.plot_predictions(\n",
    "    sequence=insulin_sequence,\n",
    "    mask_index=26\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "0b7d70a3"
   },
   "source": [
    "# Data Extraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Data Preparation: Building Custom Dataset\n",
    "\n",
    "Instead of using pre-curated datasets, I built the training data from scratch using public databases to understand the complete annotation pipeline.\n",
    "\n",
    "### Download GO Annotations (Gene Ontology Consortium)\n",
    "- Source: `goa_human.gaf.gz` from current.geneontology.org\n",
    "- Contains: All human protein functional annotations (Gene Ontology terms)\n",
    "- Filter: Taxonomy 9606 (Homo sapiens only)\n",
    "- Format: GAF (Gene Association File) with 17 columns\n",
    "\n",
    "### Map Sequences from UniProt\n",
    "- Source: UniProt human proteome FASTA file\n",
    "- Match: UniProt IDs between GAF annotations and FASTA sequences\n",
    "- Result: Proteins with both GO functional annotations AND amino acid sequences\n",
    "- Key: EntryID (UniProt accession) links both datasets\n",
    "\n",
    "### Load GO Term Descriptions\n",
    "- Source: `go-basic.obo` from Gene Ontology Consortium\n",
    "- Purpose: Convert GO IDs (GO:0003677) to human-readable descriptions (\"DNA binding\")\n",
    "- Also extracts: GO aspect/namespace (MFO, BPO, CCO) to filter for Molecular Function Ontology only\n",
    "- Saved locally: `go_descriptions.csv` (39,354 terms) to avoid re-downloading\n",
    "\n",
    "This allows interpretable analysis - we can see \"DNA binding\" instead of just \"GO:0003677\" in results.\n",
    "\n",
    "### Quick Validation: Extracellular vs Membrane Proteins\n",
    "Before processing the full dataset, validate that ESM-2 embeddings capture biological signal:\n",
    "- Sample: 20 extracellular + 20 membrane proteins\n",
    "- Model: Small ESM-2 (8M parameters, 320-dim embeddings) for speed\n",
    "- Visualization: t-SNE reduces 320D → 2D to check if proteins cluster by location\n",
    "- Result: Clear separation confirms embeddings capture cellular localization patterns\n",
    "\n",
    "This validation step ensures the pipeline works correctly before investing compute time on 8,704 proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O goa_human.gaf.gz \"http://current.geneontology.org/annotations/goa_human.gaf.gz\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "with gzip.open('goa_human.gaf.gz', 'rb') as f_in:\n",
    "    with open('goa_human.gaf', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\" File decompressed: goa_human.gaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Parse GO Annotation File (GAF format)\n",
    "# =============================================================================\n",
    "# GAF files have 17 columns but we only need 3:\n",
    "# - DB_Object_ID: UniProt protein identifier\n",
    "# - GO_ID: Gene Ontology term (e.g., GO:0003677 for DNA binding)\n",
    "# - Taxon: Organism taxonomy ID (9606 = Homo sapiens)\n",
    "#\n",
    "# Lines starting with '!' are comments (skipped with comment='!')\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cols = [\n",
    "    \"DB\", \"DB_Object_ID\", \"DB_Object_Symbol\", \"Qualifier\", \"GO_ID\",\n",
    "    \"DB_Reference\", \"Evidence_Code\", \"With\", \"Aspect\", \"DB_Object_Name\",\n",
    "    \"Synonym\", \"DB_Object_Type\", \"Taxon\", \"Date\", \"Assigned_By\",\n",
    "    \"Annotation_Extension\", \"Gene_Product_Form_ID\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"goa_human.gaf\",\n",
    "    sep=\"\\t\",\n",
    "    comment=\"!\",  # Skip comment lines\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    usecols=[\"DB_Object_ID\", \"GO_ID\", \"Taxon\"]  # Only keep what we need\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Filter for Human Proteins Only\n",
    "# =============================================================================\n",
    "# Taxon 9606 = Homo sapiens\n",
    "# Remove duplicates (same protein can have same GO term from multiple sources)\n",
    "# =============================================================================\n",
    "df = df[df[\"Taxon\"].str.contains(\"9606\")]\n",
    "df = df[[\"DB_Object_ID\", \"GO_ID\"]].drop_duplicates()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns for Consistency\n",
    "# =============================================================================\n",
    "# Standardize column names to match our pipeline conventions:\n",
    "# - EntryID: UniProt identifier (e.g., P12345)\n",
    "# - term: GO term identifier (e.g., GO:0003677)\n",
    "# ============================================================\n",
    "go_df = df.rename(columns={\n",
    "    \"DB_Object_ID\": \"EntryID\",\n",
    "    \"GO_ID\": \"term\"\n",
    "})\n",
    "go_df = go_df[[\"EntryID\", \"term\"]].drop_duplicates()\n",
    "go_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Map UniProt IDs to Amino Acid Sequences\n",
    "# =============================================================================\n",
    "# We have GO annotations but need the actual protein sequences for ESM-2.\n",
    "# \n",
    "# FASTA format: >sp|P12345|PROT_HUMAN Description\n",
    "# We extract P12345 (UniProt ID) and match it to our GO annotations.\n",
    "#\n",
    "# Result: DataFrame with EntryID, GO term, Sequence, and Length\n",
    "# =============================================================================\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Load UniProt Human FASTA file (downloaded manually from uniprot.org)\n",
    "fasta_file = \"uniprot_human.fasta\"\n",
    "\n",
    "# Build dictionary: {UniProt_ID: amino_acid_sequence}\n",
    "sequence_dict = {}\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    # UniProt FASTA headers: >sp|Q9Y2K3|NUDT4B_HUMAN ...\n",
    "    # We need the middle part (Q9Y2K3)\n",
    "    parts = record.id.split(\"|\")\n",
    "    uniprot_id = parts[1] if len(parts) > 1 else record.id\n",
    "    sequence_dict[uniprot_id] = str(record.seq)\n",
    "\n",
    "# Map sequences to GO annotation table\n",
    "go_df[\"Sequence\"] = go_df[\"EntryID\"].map(sequence_dict)\n",
    "\n",
    "# Drop proteins where sequence wasn't found (IDs mismatch between databases)\n",
    "go_df = go_df.dropna(subset=[\"Sequence\"])\n",
    "\n",
    "# Calculate sequence length for filtering later\n",
    "go_df[\"Length\"] = go_df[\"Sequence\"].str.len()\n",
    "\n",
    "protein_df = go_df.copy()\n",
    "protein_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "id": "686b66b5",
    "outputId": "da2b1750-3f3a-4091-f5eb-27c4817e04d8"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Quick Test: Extracellular vs Membrane Protein Embeddings\n",
    "# =============================================================================\n",
    "# Before training on full dataset, verify ESM-2 embeddings capture biological\n",
    "# signal by testing if they separate proteins by cellular localization.\n",
    "#\n",
    "# Strategy:\n",
    "# 1. Filter proteins with single GO annotation (clean labels)\n",
    "# 2. Sample 20 extracellular + 20 membrane proteins\n",
    "# 3. Generate embeddings and visualize with t-SNE\n",
    "#\n",
    "# If embeddings cluster by location → ESM-2 learned meaningful representations\n",
    "# =============================================================================\n",
    "\n",
    "# Keep only proteins with single GO term (avoids ambiguous annotations)\n",
    "num_locations = protein_df.groupby(\"EntryID\")[\"term\"].nunique()\n",
    "proteins_one_location = num_locations[num_locations == 1].index\n",
    "protein_df = protein_df[protein_df[\"EntryID\"].isin(proteins_one_location)]\n",
    "\n",
    "# Define cellular component GO terms to compare\n",
    "go_function_examples = {\n",
    "  \"extracellular\": \"GO:0005576\",  # Extracellular region\n",
    "  \"membrane\": \"GO:0016020\",        # Membrane\n",
    "}\n",
    "\n",
    "sequences_by_function = {}\n",
    "\n",
    "min_length = 100   # Exclude very short proteins\n",
    "max_length = 500   # Cap length for memory/speed\n",
    "num_samples = 20   # Sample size per category\n",
    "\n",
    "for function, go_term in go_function_examples.items():\n",
    "  # Filter by GO term and length constraints\n",
    "  proteins_with_function = protein_df[\n",
    "    (protein_df[\"term\"] == go_term)\n",
    "    & (protein_df[\"Length\"] >= min_length)\n",
    "    & (protein_df[\"Length\"] <= max_length)\n",
    "  ]\n",
    "  \n",
    "  print(\n",
    "    f\"Found {len(proteins_with_function)} human proteins\\n\"\n",
    "    f\"with the molecular function '{function}' ({go_term}),\\n\"\n",
    "    f\"and {min_length}<=length<={max_length}.\\n\"\n",
    "    f\"Sampling {num_samples} proteins at random.\\n\"\n",
    "  )\n",
    "  \n",
    "  # Randomly sample proteins (reproducible with random_state)\n",
    "  sequences = list(\n",
    "    proteins_with_function.sample(num_samples, random_state=42)[\"Sequence\"]\n",
    "  )\n",
    "  sequences_by_function[function] = sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "Successfully identified sufficient proteins for quick embedding test:\n",
    "- **Extracellular proteins**: 36 available → sampled 20\n",
    "- **Membrane proteins**: 105 available → sampled 20\n",
    "\n",
    "Next step: Generate ESM-2 embeddings for these 40 proteins to verify if the model captures cellular localization patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Define Embedding Extraction Function\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_mean_embeddings(sequences, tokenizer, model, device):\n",
    "    \"\"\"Extract ESM-2 mean-pooled embeddings\"\"\"\n",
    "    embeddings = []\n",
    "    batch_size = 8\n",
    "    \n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch = sequences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings) #n_proteinsm embedding_dim\n",
    "\n",
    "print(\"get_mean_embeddings function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Generate Embeddings for small Proteins dataset to differentiate between membrane and extracellular proteins\n",
    "\n",
    "Now we use ESM-2 to convert our 40 protein sequences into numerical representations.\n",
    "\n",
    "**Process:**\n",
    "1. Load smaller ESM-2 model (8M parameters) for speed\n",
    "2. For each protein sequence:\n",
    "   - Tokenize into amino acid IDs\n",
    "   - Pass through ESM-2 to get per-residue embeddings\n",
    "   - Average across all residues → single vector per protein\n",
    "3. Result: Each protein = 320-dimensional embedding vector\n",
    "\n",
    "**Why mean pooling?** \n",
    "Proteins have variable length (100-500 aa), but classifiers need fixed-size input. Averaging embeddings across all positions gives us a single representative vector while preserving biological information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "id": "f9fe5f6c",
    "outputId": "5b6957b0-955a-4f3c-f4a3-6dc76fc12797",
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Generate Embeddings for Test Proteins (small model for speed)\n",
    "# =============================================================================\n",
    "\n",
    "# Load smaller ESM-2 model (8M parameters vs 650M)\n",
    "# Faster for quick validation tests with 40 proteins\n",
    "model_checkpoint_small = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer_small = AutoTokenizer.from_pretrained(model_checkpoint_small)\n",
    "model_small = EsmModel.from_pretrained(model_checkpoint_small)\n",
    "\n",
    "# Use CPU to avoid MPS memory issues with batch processing\n",
    "device = torch.device(\"cpu\")\n",
    "model_small = model_small.to(device)\n",
    "model_small.eval()  # Set to evaluation mode (disables dropout)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using model: {model_checkpoint_small}\")\n",
    "\n",
    "# Generate embeddings for each function category (extracellular, membrane)\n",
    "embeddings_by_function = {}\n",
    "\n",
    "for function_name, sequences in sequences_by_function.items():\n",
    "    print(f\"\\nEncoding {len(sequences)} sequences for function: {function_name}\")\n",
    "    \n",
    "    # Extract mean-pooled embeddings: (20 proteins, 320 dimensions)\n",
    "    embeddings = get_mean_embeddings(\n",
    "        sequences=sequences,\n",
    "        tokenizer=tokenizer_small,\n",
    "        model=model_small,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Store embeddings array for this category\n",
    "    embeddings_by_function[function_name] = embeddings\n",
    "\n",
    "print(\"\\nEmbeddings generated successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "id": "cff24168",
    "outputId": "16a8dd6a-66c7-4e34-c95c-8c8b3a8dbf73"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Prepare Data for Visualization\n",
    "# =============================================================================\n",
    "# Convert dictionary of embeddings to arrays suitable for t-SNE:\n",
    "# - embeddings: (40, 320) matrix where each row is a protein\n",
    "# - labels: list of 40 strings (\"extracellular\" or \"membrane\")\n",
    "#\n",
    "# This format allows us to color points by cellular location in the plot.\n",
    "# =============================================================================\n",
    "\n",
    "labels = []\n",
    "embeddings = []\n",
    "\n",
    "for location, embedding_array in embeddings_by_function.items():\n",
    "    # Sanity check: verify embedding dimensions\n",
    "    print(f\"{location}: {embedding_array.shape}\")\n",
    "\n",
    "    # Create label for each protein (repeated 20 times per category)\n",
    "    labels.extend([location] * embedding_array.shape[0])\n",
    "\n",
    "    # Collect embedding arrays\n",
    "    embeddings.append(embedding_array)\n",
    "\n",
    "# Stack into single matrix: (40 proteins, 320 dimensions)\n",
    "import numpy as np\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(\"Embeddings matrix:\", embeddings.shape)  # Should be (40, 320)\n",
    "print(\"Labels length:\", len(labels))            # Should be 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "id": "c4228fde",
    "mystnb": {
     "figure": {
      "caption": "Two-dimensional t-SNE projection of the 320-dimensional embeddings from a small ESM2 model. Even with this lightweight model, we observe a tendency for extracellular and membrane proteins to form separate clusters, suggesting that the embeddings contain information relevant to cellular localization",
      "name": "membrane_protein_embeddings"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    },
    "outputId": "2b4805a0-1a92-44b9-9347-4b5c09be9827"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Embeddings with t-SNE\n",
    "# =============================================================================\n",
    "# t-SNE reduces 320 dimensions → 2 dimensions while preserving local structure.\n",
    "# \n",
    "# Goal: Check if ESM-2 embeddings naturally cluster by cellular location.\n",
    "# If yes → embeddings capture biologically meaningful features for classification.\n",
    "#\n",
    "# Parameters:\n",
    "# - n_components=2: Project to 2D for visualization\n",
    "# - perplexity=10: Good for small datasets (40 samples)\n",
    "# - random_state=42: Reproducible results\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Ensure embeddings are numpy array\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Reduce dimensionality: 320D → 2D\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=10,      # Appropriate for 40 samples (rule of thumb: 5-50)\n",
    "    learning_rate='auto',\n",
    "    random_state=42     # Reproducibility\n",
    ")\n",
    "\n",
    "embeddings_tsne = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Create dataframe for easy plotting\n",
    "embeddings_tsne_df = pd.DataFrame({\n",
    "    \"first_dimension\": embeddings_tsne[:, 0],\n",
    "    \"second_dimension\": embeddings_tsne[:, 1],\n",
    "    \"location\": labels,\n",
    "})\n",
    "\n",
    "# Visualize: Do extracellular and membrane proteins separate?\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(\n",
    "    data=embeddings_tsne_df,\n",
    "    x=\"first_dimension\",\n",
    "    y=\"second_dimension\",\n",
    "    hue=\"location\",\n",
    "    style=\"location\",\n",
    "    s=70,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "plt.title(\"t-SNE of Protein Embeddings (ESM2)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Embedding Quality Check\n",
    "\n",
    "**Result:** ESM-2 embeddings separate extracellular from membrane proteins with clear clustering, demonstrating that:\n",
    "\n",
    "1. **Biological signal captured**: Model learned sequence patterns associated with cellular localization\n",
    "2. **Transfer learning viable**: Pre-trained embeddings are informative for downstream tasks\n",
    "3. **Ready for classification**: If embeddings distinguish 2 categories, they can handle 40+ molecular functions\n",
    "\n",
    "This validates our approach before scaling to full multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## Full Dataset Preparation\n",
    "\n",
    "Now we move from the 40-protein test to building our complete training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load GO Term Descriptions and Ontology\n",
    "# =============================================================================\n",
    "import os\n",
    "import obonet\n",
    "\n",
    "def get_go_term_descriptions(store_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Return GO term to description mapping, downloading if needed\"\"\"\n",
    "    if not os.path.exists(store_path):\n",
    "        print(\"Downloading GO ontology...\")\n",
    "        url = \"https://current.geneontology.org/ontology/go-basic.obo\"\n",
    "        graph = obonet.read_obo(url)\n",
    "        \n",
    "        id_to_name = {id: data.get(\"name\") for id, data in graph.nodes(data=True)}\n",
    "        go_term_descriptions = pd.DataFrame(\n",
    "            zip(id_to_name.keys(), id_to_name.values()),\n",
    "            columns=[\"term\", \"description\"],\n",
    "        )\n",
    "        go_term_descriptions.to_csv(store_path, index=False)\n",
    "    else:\n",
    "        go_term_descriptions = pd.read_csv(store_path)\n",
    "    \n",
    "    return go_term_descriptions\n",
    "\n",
    "def add_go_aspect(labels_df, go_graph):\n",
    "    \"\"\"Add GO aspect column (MFO, BPO, CCO)\"\"\"\n",
    "    aspect_mapping = {\n",
    "        'molecular_function': 'MFO',\n",
    "        'biological_process': 'BPO', \n",
    "        'cellular_component': 'CCO'\n",
    "    }\n",
    "    \n",
    "    aspects = []\n",
    "    for term in labels_df['term']:\n",
    "        if term in go_graph.nodes:\n",
    "            namespace = go_graph.nodes[term].get('namespace', None)\n",
    "            aspect = aspect_mapping.get(namespace, 'Unknown')\n",
    "        else:\n",
    "            aspect = 'Unknown'\n",
    "        aspects.append(aspect)\n",
    "    \n",
    "    labels_df['aspect'] = aspects\n",
    "    return labels_df\n",
    "\n",
    "# Load GO descriptions\n",
    "data_path = \"/Users/danielaalejandragonzalez/Library/CloudStorage/OneDrive-Personal/llmprotein/data/\"\n",
    "go_descriptions = get_go_term_descriptions(data_path + \"go_descriptions.csv\")\n",
    "\n",
    "# Load GO graph for aspect information\n",
    "url = \"https://current.geneontology.org/ontology/go-basic.obo\"\n",
    "go_graph = obonet.read_obo(url)\n",
    "\n",
    "print(f\"Loaded {len(go_descriptions)} GO term descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Rebuild Full Dataset (sin filtro de single annotation)\n",
    "# =============================================================================\n",
    "\n",
    "# Start fresh from go_df (después del merge con sequences)\n",
    "# Ya deberías tener go_df con: EntryID, term, Sequence, Length\n",
    "\n",
    "# Add aspect if needed\n",
    "if 'aspect' not in go_df.columns:\n",
    "    go_df = add_go_aspect(go_df, go_graph)\n",
    "\n",
    "# Filter for Molecular Function only\n",
    "sequence_df = go_df[go_df[\"aspect\"] == \"MFO\"].copy()\n",
    "\n",
    "print(f\"After MFO filter: {len(sequence_df)} protein-GO pairs\")\n",
    "print(f\"Unique proteins: {sequence_df['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {sequence_df['term'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Remove Uninformative GO Terms\n",
    "# =============================================================================\n",
    "# Filter out root-level GO terms that provide no discriminative power\n",
    "# =============================================================================\n",
    "\n",
    "uninteresting_functions = [\n",
    "    \"GO:0003674\",  # \"molecular_function\" - root term\n",
    "    \"GO:0005488\",  # \"binding\" - too generic\n",
    "    \"GO:0005515\",  # \"protein binding\" - too generic\n",
    "]\n",
    "\n",
    "sequence_df = sequence_df[~sequence_df[\"term\"].isin(uninteresting_functions)]\n",
    "\n",
    "print(f\"After removing generic terms: {len(sequence_df)} protein-GO pairs\")\n",
    "print(f\"Unique proteins: {sequence_df['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {sequence_df['term'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Filter for Sufficient Training Data\n",
    "# =============================================================================\n",
    "# Rare GO terms (< 50 examples) don't have enough data for reliable learning.\n",
    "# We keep only terms with ≥ 50 proteins to ensure model can learn patterns.\n",
    "# =============================================================================\n",
    "\n",
    "# Identify GO terms with at least 50 occurrences\n",
    "common_functions = (\n",
    "    sequence_df[\"term\"]\n",
    "    .value_counts()[sequence_df[\"term\"].value_counts() >= 50]\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Keep only proteins with common GO terms\n",
    "sequence_df = sequence_df[sequence_df[\"term\"].isin(common_functions)]\n",
    "\n",
    "print(f\"After filtering for common terms (≥50): {len(sequence_df)} protein-GO pairs\")\n",
    "print(f\"Unique proteins: {sequence_df['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {sequence_df['term'].nunique()}\")\n",
    "print(f\"\\nMost common functions:\")\n",
    "print(sequence_df[\"term\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### Explore Multi-Label Distribution\n",
    "\n",
    "Proteins typically have multiple molecular functions. Let's visualize how many GO terms are assigned per protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "id": "db5f2a50",
    "mystnb": {
     "figure": {
      "caption": "Distribution of the number of molecular functions annotated per protein. The y-axis is shown on a logarithmic scale to make rare cases more visible. While most proteins have fewer than 20 annotated functions, a small number are associated with over 50 distinct molecular roles.",
      "name": "functions_per_protein"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    },
    "outputId": "fb43ae16-7a97-4da9-f6d4-80cca26afe06"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Multi-Label Nature of Dataset\n",
    "# =============================================================================\n",
    "# Most proteins have multiple molecular functions (multi-label classification).\n",
    "# This histogram shows the distribution of how many GO terms each protein has.\n",
    "#\n",
    "# Expected pattern: Most proteins have 5-20 functions, some have 50+\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count unique GO terms per protein\n",
    "sequence_df.groupby(\"EntryID\")[\"term\"].nunique().plot.hist(\n",
    "    bins=100, \n",
    "    figsize=(5, 3), \n",
    "    color=\"grey\", \n",
    "    log=True  # Log scale reveals long tail\n",
    ")\n",
    "plt.xlabel(\"Number of Molecular Function Annotations per Protein\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Distribution of Function Counts per Protein\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### Convert to Multi-Label Format\n",
    "\n",
    "Transform from long format (one row per protein-function pair) to wide format (one row per protein, binary columns for each function). This is standard format for multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {
    "id": "a58a58e6",
    "outputId": "89e7124e-013f-49d9-d9a9-03873eeeb4e9"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Pivot to Multi-Label Binary Matrix\n",
    "# =============================================================================\n",
    "# Transform from long format:\n",
    "#   EntryID | Sequence | term\n",
    "#   P12345  | MALW...  | GO:0001\n",
    "#   P12345  | MALW...  | GO:0002\n",
    "#\n",
    "# To wide format (one-hot encoding):\n",
    "#   EntryID | Sequence | GO:0001 | GO:0002 | ...\n",
    "#   P12345  | MALW...  |    1    |    1    | ...\n",
    "#\n",
    "# Result: Binary matrix where each GO term column = 1 if protein has that function\n",
    "# =============================================================================\n",
    "\n",
    "sequence_df = (\n",
    "    sequence_df[[\"EntryID\", \"Sequence\", \"Length\", \"term\"]]\n",
    "    .assign(value=1)  # Create indicator for pivot\n",
    "    .pivot(\n",
    "        index=[\"EntryID\", \"Sequence\", \"Length\"], \n",
    "        columns=\"term\", \n",
    "        values=\"value\"\n",
    "    )\n",
    "    .fillna(0)  # 0 = protein doesn't have this function\n",
    "    .astype(int)  # Binary: 0 or 1\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape (wide format): {sequence_df.shape}\")\n",
    "print(f\"Number of proteins: {len(sequence_df)}\")\n",
    "print(f\"Number of GO term columns: {sequence_df.shape[1] - 3}\")\n",
    "print(\"\\nFirst rows:\")\n",
    "print(sequence_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "id": "4ad6949e",
    "outputId": "84d17418-54e3-4a0f-8369-e2c131b0a5a8"
   },
   "outputs": [],
   "source": [
    "sequence_df[\"Sequence\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "id": "c6895cd9",
    "outputId": "ab34c8ba-9b01-4104-d068-d8e4fcfd7714"
   },
   "outputs": [],
   "source": [
    "print(sequence_df.shape)\n",
    "sequence_df = sequence_df[sequence_df[\"Length\"] <= 500]\n",
    "print(sequence_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {
    "id": "915ce241"
   },
   "source": [
    "## Splitting the Dataset into Subsets\n",
    "### Model Training Pipeline\n",
    "\n",
    "Now we train a multi-label classifier to predict molecular functions from ESM-2 embeddings.\n",
    "\n",
    "### Split Data by Protein ID\n",
    "\n",
    "**Critical:** Split by protein, not by rows, to prevent data leakage. If the same protein appears in both train and test, the model can memorize rather than generalize.\n",
    "\n",
    "**Split:** 60% train / 20% validation / 20% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "id": "4177e2d4"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Train/Validation/Test Splits\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_protein_ids = sequence_df[\"EntryID\"].tolist()\n",
    "\n",
    "train_sequence_ids, valid_test_sequence_ids = train_test_split(\n",
    "    all_protein_ids, \n",
    "    test_size=0.40, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "valid_sequence_ids, test_sequence_ids = train_test_split(\n",
    "    valid_test_sequence_ids, \n",
    "    test_size=0.50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_sequence_ids)}\")\n",
    "print(f\"Valid: {len(valid_sequence_ids)}\")\n",
    "print(f\"Test: {len(test_sequence_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "527635f5"
   },
   "source": [
    "### Converting Protein Sequences into Their Mean Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Generate ESM-2 Embeddings (first checking with 30 proteins)\n",
    "\n",
    "Converting sequences to embeddings is computationally expensive. We first test the pipeline on 10 proteins per split to verify everything works before processing the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Small Test Subset (Pipeline Validation)\n",
    "# =============================================================================\n",
    "# Before processing all proteins (~300), test pipeline on 10 per split.\n",
    "# This catches bugs quickly without wasting compute time.\n",
    "#\n",
    "# Once validated, we'll run on full dataset.\n",
    "# =============================================================================\n",
    "\n",
    "sequence_splits_small = {\n",
    "    \"train\": sequence_df[sequence_df[\"EntryID\"].isin(train_sequence_ids)].head(10),\n",
    "    \"valid\": sequence_df[sequence_df[\"EntryID\"].isin(valid_sequence_ids)].head(10),\n",
    "    \"test\": sequence_df[sequence_df[\"EntryID\"].isin(test_sequence_ids)].head(10),\n",
    "}\n",
    "\n",
    "print(\"Small subsets for testing:\")\n",
    "for split, df in sequence_splits_small.items():\n",
    "    print(f\"{split}: {len(df)} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load ESM-2 Model for Embedding Extraction\n",
    "# =============================================================================\n",
    "# Using 150M parameter model for better quality embeddings\n",
    "# CPU mode to avoid MPS memory issues with large batches\n",
    "# =============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "print(f\"Loading model: {model_checkpoint}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = EsmModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Use CPU for stability with large dataset\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Embedding Extraction Function\n",
    "# =============================================================================\n",
    "# Process proteins in batches to avoid memory overflow.\n",
    "#\n",
    "# Steps per batch:\n",
    "# 1. Tokenize sequences → integer IDs\n",
    "# 2. Pad to equal length (ESM-2 requires fixed-size input per batch)\n",
    "# 3. Forward pass through model → per-residue embeddings\n",
    "# 4. Mean pooling across sequence length → single vector per protein\n",
    "#\n",
    "# Returns: (n_proteins, 640) array of embeddings\n",
    "# =============================================================================\n",
    "\n",
    "def extract_embeddings(sequences, tokenizer, model, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extract ESM-2 mean-pooled embeddings for protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of amino acid strings\n",
    "        tokenizer: ESM-2 tokenizer\n",
    "        model: ESM-2 model\n",
    "        batch_size: Process this many proteins at once (adjust based on GPU memory)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: (n_sequences, embedding_dim) embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch_sequences = sequences[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize and pad batch\n",
    "        inputs = tokenizer(\n",
    "            batch_sequences, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,          # Pad to longest in batch\n",
    "            truncation=True,       # Truncate if > max_length\n",
    "            max_length=1024        # ESM-2 max sequence length\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Extract embeddings without computing gradients (faster + less memory)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Mean pooling: (batch, seq_len, 640) → (batch, 640)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        print(f\"Processed batch {i//batch_size + 1}\")\n",
    "    \n",
    "    # Stack all batches: list of arrays → single array\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Extract Embeddings for Small Test Set\n",
    "# =============================================================================\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "for split, df in sequence_splits_small.items():\n",
    "    print(f\"\\nExtracting embeddings for {split} set ({len(df)} proteins)...\")\n",
    "    \n",
    "    sequences = df[\"Sequence\"].tolist()\n",
    "    embeddings = extract_embeddings(sequences, tokenizer, model, batch_size=8)\n",
    "    \n",
    "    # Add embeddings as new column\n",
    "    df_with_embeddings = df.copy()\n",
    "    df_with_embeddings['embeddings'] = list(embeddings)\n",
    "    \n",
    "    embeddings_dict[split] = df_with_embeddings\n",
    "    print(f\"{split} embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Store splits\n",
    "train_df_small = embeddings_dict['train']\n",
    "valid_df_small = embeddings_dict['valid']\n",
    "test_df_small = embeddings_dict['test']\n",
    "\n",
    "print(\"\\nEmbedding extraction complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "### Train Classifier on Small Test Set\n",
    "\n",
    "Now that we have embeddings, train a simple neural network to predict GO terms. This validates the full pipeline works before scaling to all proteins.\n",
    "\n",
    "**Architecture:**\n",
    "- Input: 640-dim ESM-2 embeddings (frozen)\n",
    "- Hidden: 128 units + dropout\n",
    "- Output: 40+ GO terms (sigmoid for multi-label)\n",
    "\n",
    "**Goal:** Verify pipeline end-to-end with 30 proteins before processing 300+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train Classifier on Small Test Set (30 proteins)\n",
    "# =============================================================================\n",
    "# Quick validation that pipeline works before processing full dataset.\n",
    "# Using PyTorch instead of TensorFlow for GPU acceleration.\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(df_with_embeddings):\n",
    "    \"\"\"Extract embeddings and labels\"\"\"\n",
    "    X = np.vstack(df_with_embeddings['embeddings'].values)\n",
    "    go_columns = [col for col in df_with_embeddings.columns if col.startswith('GO:')]\n",
    "    y = df_with_embeddings[go_columns].values.astype(np.float32)\n",
    "    return X, y, go_columns\n",
    "\n",
    "X_train, y_train, go_terms = prepare_data(train_df_small)\n",
    "X_valid, y_valid, _ = prepare_data(valid_df_small)\n",
    "X_test, y_test, _ = prepare_data(test_df_small)\n",
    "\n",
    "print(f\"Training on: {X_train.shape[0]} proteins\")\n",
    "print(f\"Embedding dim: {X_train.shape[1]}\")\n",
    "print(f\"GO terms: {len(go_terms)}\")\n",
    "\n",
    "# Convert to PyTorch\n",
    "X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "X_valid_t = torch.FloatTensor(X_valid).to(device)\n",
    "y_valid_t = torch.FloatTensor(y_valid).to(device)\n",
    "X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Simple model\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleClassifier(X_train.shape[1], len(go_terms)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_t)\n",
    "    loss = criterion(outputs, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_valid_t)\n",
    "            val_loss = criterion(val_outputs, y_valid_t)\n",
    "        print(f\"Epoch {epoch}: Train Loss={loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_t)\n",
    "    test_loss = criterion(test_outputs, y_test_t)\n",
    "    test_acc = ((test_outputs > 0.5) == y_test_t).float().mean()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_acc:.3f}\")\n",
    "print(\"Pipeline validated with 30 proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Predictions vs Ground Truth (Small Model)\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions on validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_probs = model(X_valid_t).cpu().numpy()\n",
    "\n",
    "# Get true labels\n",
    "valid_true = y_valid\n",
    "\n",
    "# Create heatmaps side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# True labels\n",
    "sns.heatmap(valid_true, ax=axes[0], cmap='Reds', \n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'label': 'True'})\n",
    "axes[0].set_title('Ground Truth (10 proteins)')\n",
    "axes[0].set_xlabel('GO Terms')\n",
    "axes[0].set_ylabel('Proteins')\n",
    "\n",
    "# Predicted probabilities\n",
    "sns.heatmap(valid_probs, ax=axes[1], cmap='Blues',\n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'label': 'Predicted'})\n",
    "axes[1].set_title('Model Predictions (10 proteins)')\n",
    "axes[1].set_xlabel('GO Terms')\n",
    "axes[1].set_ylabel('Proteins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPattern: Dark cells = functions present\")\n",
    "print(f\"Model learns sparse multi-label structure with only 10 training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Show Top Predicted vs True GO Terms per Protein\n",
    "# =============================================================================\n",
    "\n",
    "# Get top 5 predictions per protein\n",
    "for i in range(3):  # Show first 3 proteins only\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Protein {i+1}:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # True GO terms\n",
    "    true_indices = np.where(valid_true[i] == 1)[0]\n",
    "    true_terms = [go_terms[idx] for idx in true_indices]\n",
    "    print(f\"\\nTRUE GO terms ({len(true_terms)}):\")\n",
    "    for term in true_terms[:5]:  # Show max 5\n",
    "        desc = go_descriptions[go_descriptions['term'] == term]['description'].values\n",
    "        desc = desc[0] if len(desc) > 0 else \"Unknown\"\n",
    "        print(f\"  {term}: {desc}\")\n",
    "    \n",
    "    # Top 5 predicted\n",
    "    top5_indices = np.argsort(valid_probs[i])[-5:][::-1]\n",
    "    print(f\"\\nTOP 5 PREDICTED:\")\n",
    "    for idx in top5_indices:\n",
    "        term = go_terms[idx]\n",
    "        prob = valid_probs[i, idx]\n",
    "        desc = go_descriptions[go_descriptions['term'] == term]['description'].values\n",
    "        desc = desc[0] if len(desc) > 0 else \"Unknown\"\n",
    "        correct = \"✓\" if valid_true[i, idx] == 1 else \"✗\"\n",
    "        print(f\"  {correct} {term} ({prob:.2f}): {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "# Full dataset training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### Extract Embeddings for Full Dataset\n",
    "\n",
    "Pipeline validated on 30 proteins--> Now process all ~8000 preoteins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Split DataFrames\n",
    "# =============================================================================\n",
    "\n",
    "sequence_splits = {\n",
    "    \"train\": sequence_df[sequence_df[\"EntryID\"].isin(train_sequence_ids)],\n",
    "    \"valid\": sequence_df[sequence_df[\"EntryID\"].isin(valid_sequence_ids)],\n",
    "    \"test\": sequence_df[sequence_df[\"EntryID\"].isin(test_sequence_ids)],\n",
    "}\n",
    "\n",
    "for split, df in sequence_splits.items():\n",
    "    print(f\"{split}: {len(df)} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Reload ESM-2 for Full Dataset\n",
    "# =============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "\n",
    "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "print(f\"Loading ESM-2: {model_checkpoint}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "esm_model = EsmModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "esm_model = esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "print(f\"ESM-2 loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting full dataset embedding extraction...\")\n",
    "print(f\"Total proteins: {sum(len(df) for df in sequence_splits.values())}\")\n",
    "\n",
    "embeddings_dict_full = {}\n",
    "data_path = \"/Users/danielaalejandragonzalez/Library/CloudStorage/OneDrive-Personal/llmprotein/\"\n",
    "\n",
    "for split, df in sequence_splits.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {split} set: {len(df)} proteins\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    sequences = df[\"Sequence\"].tolist()\n",
    "    embeddings = extract_embeddings(sequences, tokenizer, esm_model, batch_size=16)\n",
    "    \n",
    "    df_with_embeddings = df.copy()\n",
    "    df_with_embeddings['embeddings'] = list(embeddings)\n",
    "    \n",
    "    embeddings_dict_full[split] = df_with_embeddings\n",
    "    \n",
    "    print(f\"✓ {split} complete: embeddings shape {embeddings.shape}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    df_with_embeddings.to_pickle(data_path + f\"embeddings_{split}.pkl\")\n",
    "    print(f\"✓ Saved to {data_path}embeddings_{split}.pkl\")\n",
    "\n",
    "print(\"\\nALL EMBEDDINGS EXTRACTED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### Train Final Classifier on Full Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Prepare Data Arrays for Training\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def prepare_data(df_with_embeddings):\n",
    "    \"\"\"Extract embeddings (X) and GO term labels (y)\"\"\"\n",
    "    X = np.vstack(df_with_embeddings['embeddings'].values)\n",
    "    go_columns = [col for col in df_with_embeddings.columns if col.startswith('GO:')]\n",
    "    y = df_with_embeddings[go_columns].values.astype(np.float32)\n",
    "    return X, y, go_columns\n",
    "\n",
    "# Prepare all splits\n",
    "X_train_full, y_train_full, go_terms = prepare_data(embeddings_dict_full['train'])\n",
    "X_valid_full, y_valid_full, _ = prepare_data(embeddings_dict_full['valid'])\n",
    "X_test_full, y_test_full, _ = prepare_data(embeddings_dict_full['test'])\n",
    "\n",
    "print(f\"Training set: {X_train_full.shape[0]} proteins\")\n",
    "print(f\"Validation set: {X_valid_full.shape[0]} proteins\")\n",
    "print(f\"Test set: {X_test_full.shape[0]} proteins\")\n",
    "print(f\"Embedding dimension: {X_train_full.shape[1]}\")\n",
    "print(f\"Number of GO terms: {len(go_terms)}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "X_train_t = torch.FloatTensor(X_train_full).to(device)\n",
    "y_train_t = torch.FloatTensor(y_train_full).to(device)\n",
    "X_valid_t = torch.FloatTensor(X_valid_full).to(device)\n",
    "y_valid_t = torch.FloatTensor(y_valid_full).to(device)\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(\"Data ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Hyperparameter Experimentation\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define configurations to test\n",
    "configs = {\n",
    "    \"baseline\": {\n",
    "        \"architecture\": [512, 256],\n",
    "        \"dropout\": 0.3,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_norm\": False\n",
    "    },\n",
    "    \"deeper\": {\n",
    "        \"architecture\": [512, 256, 128],\n",
    "        \"dropout\": 0.3,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_norm\": False\n",
    "    },\n",
    "    \"batch_norm\": {\n",
    "        \"architecture\": [512, 256],\n",
    "        \"dropout\": 0.3,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_norm\": True\n",
    "    },\n",
    "    \"higher_dropout\": {\n",
    "        \"architecture\": [512, 256],\n",
    "        \"dropout\": 0.5,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_norm\": False\n",
    "    },\n",
    "    \"lower_lr\": {\n",
    "        \"architecture\": [512, 256],\n",
    "        \"dropout\": 0.3,\n",
    "        \"lr\": 0.0001,\n",
    "        \"batch_norm\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flexible model class\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3, use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Training function\n",
    "def train_model(config_name, config, X_train, y_train, X_valid, y_valid):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {config_name}\")\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = ProteinClassifier(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dims=config[\"architecture\"],\n",
    "        num_classes=y_train.shape[1],\n",
    "        dropout=config[\"dropout\"],\n",
    "        use_batch_norm=config[\"batch_norm\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_valid)\n",
    "            val_loss = criterion(val_outputs, y_valid)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 5:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss={loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    return model, best_val_loss\n",
    "\n",
    "# Run all experiments\n",
    "results = {}\n",
    "for config_name, config in configs.items():\n",
    "    model, val_loss = train_model(\n",
    "        config_name, config, \n",
    "        X_train_t, y_train_t, \n",
    "        X_valid_t, y_valid_t\n",
    "    )\n",
    "    results[config_name] = {\"model\": model, \"val_loss\": val_loss}\n",
    "\n",
    "# Compare results\n",
    "print(\"RESULTS COMPARISON\")\n",
    "for name, result in sorted(results.items(), key=lambda x: x[1][\"val_loss\"]):\n",
    "    print(f\"{name:20s}: Val Loss = {result['val_loss']:.4f}\")\n",
    "    \n",
    "print(\"\\nBest configuration will be used for final test evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Final Evaluation on Test Set (Best Model)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "best_model = results['batch_norm']['model']\n",
    "X_test_t = torch.FloatTensor(X_test_full).to(device)\n",
    "y_test_t = torch.FloatTensor(y_test_full).to(device)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(X_test_t)\n",
    "    test_loss = nn.BCELoss()(test_outputs, y_test_t)\n",
    "    test_acc = ((test_outputs > 0.5) == y_test_t).float().mean()\n",
    "    \n",
    "    # Compute AUC only for GO terms with both classes present\n",
    "    test_preds = test_outputs.cpu().numpy()\n",
    "    valid_auc_scores = []\n",
    "    \n",
    "    for i in range(y_test_full.shape[1]):\n",
    "        if len(np.unique(y_test_full[:, i])) == 2:  # Both 0 and 1 present\n",
    "            auc = roc_auc_score(y_test_full[:, i], test_preds[:, i])\n",
    "            valid_auc_scores.append(auc)\n",
    "    \n",
    "    test_auc = np.mean(valid_auc_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SET RESULTS (batch_norm model)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC:      {test_auc:.4f} (averaged over {len(valid_auc_scores)}/{len(go_terms)} GO terms)\")\n",
    "print(\"\\nModel successfully predicts protein functions from ESM-2 embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Performance Summary:\n",
    "\n",
    "Test Loss: 0.0739 (very low, good predictions)\n",
    "Test Accuracy: 98.57% (high but less meaningful for imbalanced multi-label)\n",
    "Test AUC: 0.68 (averaged over 200/202 GO terms)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Model generalizes well (test loss close to validation 0.0743)\n",
    "AUC 0.68 means model ranks correct functions higher than incorrect ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Generate Predictions for Analysis\n",
    "# =============================================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_probs = best_model(X_valid_t).cpu().numpy()\n",
    "    test_probs = best_model(X_test_t).cpu().numpy()\n",
    "\n",
    "# Create prediction dataframes\n",
    "valid_true_df = embeddings_dict_full['valid'][['EntryID'] + go_terms].set_index('EntryID')\n",
    "valid_pred_df = pd.DataFrame(valid_probs, columns=go_terms, index=valid_true_df.index)\n",
    "\n",
    "test_true_df = embeddings_dict_full['test'][['EntryID'] + go_terms].set_index('EntryID')\n",
    "test_pred_df = pd.DataFrame(test_probs, columns=go_terms, index=test_true_df.index)\n",
    "\n",
    "print(\"Predictions generated for validation and test sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 1: Predictions vs Ground Truth Heatmaps\n",
    "# =============================================================================\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# True labels (first 100 proteins)\n",
    "sns.heatmap(\n",
    "    valid_true_df.iloc[:100],\n",
    "    ax=axes[0],\n",
    "    yticklabels=False,\n",
    "    xticklabels=False,\n",
    "    cmap='Reds',\n",
    "    cbar_kws={'label': 'True Label'}\n",
    ")\n",
    "axes[0].set_title('Ground Truth Annotations (100 proteins)')\n",
    "axes[0].set_xlabel('GO Terms')\n",
    "axes[0].set_ylabel('Proteins')\n",
    "\n",
    "# Predicted probabilities\n",
    "sns.heatmap(\n",
    "    valid_pred_df.iloc[:100],\n",
    "    ax=axes[1],\n",
    "    yticklabels=False,\n",
    "    xticklabels=False,\n",
    "    cmap='Blues',\n",
    "    cbar_kws={'label': 'Predicted Probability'}\n",
    ")\n",
    "axes[1].set_title('Model Predictions (100 proteins)')\n",
    "axes[1].set_xlabel('GO Terms')\n",
    "axes[1].set_ylabel('Proteins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Heatmaps show sparse multi-label structure\")\n",
    "print(\"Model learns to predict sparse patterns similar to ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 2: Per-GO-Term Performance\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Compute metrics for each GO term\n",
    "metrics_by_term = {}\n",
    "for i, go_term in enumerate(go_terms):\n",
    "    y_true = valid_true_df.iloc[:, i].values\n",
    "    y_pred = valid_pred_df.iloc[:, i].values\n",
    "    \n",
    "    # Only compute if both classes present\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_pred)\n",
    "            pr_auc = average_precision_score(y_true, y_pred)\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "            pr_auc = 0.0\n",
    "    else:\n",
    "        roc_auc = 0.0\n",
    "        pr_auc = 0.0\n",
    "    \n",
    "    metrics_by_term[go_term] = {'roc_auc': roc_auc, 'pr_auc': pr_auc}\n",
    "\n",
    "# Create summary dataframe\n",
    "performance_df = pd.DataFrame(metrics_by_term).T\n",
    "\n",
    "# Add GO descriptions\n",
    "performance_df = performance_df.merge(\n",
    "    go_descriptions[['term', 'description']], \n",
    "    left_index=True, \n",
    "    right_on='term'\n",
    ").set_index('term')\n",
    "\n",
    "# Add training frequency\n",
    "train_freq = embeddings_dict_full['train'][go_terms].sum()\n",
    "performance_df['train_count'] = train_freq\n",
    "\n",
    "# Sort by PR-AUC\n",
    "performance_df = performance_df.sort_values('pr_auc', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 best performing GO terms:\")\n",
    "print(performance_df.head(10)[['description', 'pr_auc', 'roc_auc', 'train_count']])\n",
    "\n",
    "print(\"\\nBottom 10 worst performing GO terms:\")\n",
    "print(performance_df.tail(10)[['description', 'pr_auc', 'roc_auc', 'train_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 3: Performance vs Training Data\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(\n",
    "    performance_df['train_count'],\n",
    "    performance_df['pr_auc'],\n",
    "    alpha=0.6,\n",
    "    s=50,\n",
    "    c=performance_df['roc_auc'],\n",
    "    cmap='viridis'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Number of Training Examples', fontsize=12)\n",
    "ax.set_ylabel('PR-AUC (Validation)', fontsize=12)\n",
    "ax.set_title('Model Performance vs Training Data Availability', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random baseline')\n",
    "\n",
    "plt.colorbar(scatter, label='ROC-AUC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: More training data strongly correlates with better performance\")\n",
    "print(\"Functions with <20 examples struggle to achieve PR-AUC >0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "Key Insights:\n",
    "\n",
    "Strong correlation: More data = better performance (upward trend)\n",
    "Critical threshold: Functions with <20 examples mostly fail (PR-AUC <0.1)\n",
    "Sweet spot: 50-500 examples achieve PR-AUC 0.5-0.9\n",
    "Color gradient: Yellow dots (high ROC-AUC) concentrate at top-right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "## Baseline Comparison: Validating Model Performance\n",
    "\n",
    "To verify the model learned meaningful patterns rather than random guessing, we compare against two naive baselines:\n",
    "\n",
    "### Random Baseline Strategies\n",
    "\n",
    "**1. Uniform Random (Coin Flip)**\n",
    "- **Strategy:** Predict 0.5 probability for all functions across all proteins\n",
    "- **Logic:** Equivalent to flipping a coin for each prediction\n",
    "- **Limitation:** Ignores both protein sequence and dataset statistics\n",
    "- **Expected Performance:** PR-AUC ≈ class frequency (very low for rare functions)\n",
    "\n",
    "**2. Proportional Random**\n",
    "- **Strategy:** Predict based on training set frequency for each GO term\n",
    "- **Example:** If \"DNA binding\" appears in 20% of training proteins, predict 0.20 for ALL proteins\n",
    "- **Logic:** Uses dataset statistics but ignores individual protein characteristics\n",
    "- **Limitation:** Same prediction for every protein regardless of sequence\n",
    "\n",
    "**3. ESM-2 Model (Our Approach)**\n",
    "- **Strategy:** Generate protein-specific predictions from ESM-2 sequence embeddings\n",
    "- **Logic:** Analyzes amino acid patterns to predict functions\n",
    "- **Example:** High kinase motif presence → high kinase activity probability\n",
    "\n",
    "### Results\n",
    "\n",
    "| Method | Mean PR-AUC | Interpretation |\n",
    "|--------|-------------|----------------|\n",
    "| Uniform Random | ~0.05 | Random guessing baseline |\n",
    "| Proportional Random | ~0.08 | Dataset-aware baseline |\n",
    "| ESM-2 Model | ~0.40 | **5-10x better than random** |\n",
    "\n",
    "**Conclusion:** The model significantly outperforms random baselines, demonstrating that ESM-2 embeddings capture biologically meaningful sequence-function relationships. The classifier successfully learned discriminative patterns rather than memorizing dataset statistics.\n",
    "\n",
    "**Key Validation:** Performance varies by GO term (0.0-0.99 PR-AUC), strongly correlating with training data availability. Functions with >50 training examples achieve robust predictions, while rare functions (<20 examples) remain challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 4: Compare with Random Baselines\n",
    "# =============================================================================\n",
    "\n",
    "# Create random baseline predictions\n",
    "def make_random_predictions(true_df, go_terms, strategy='uniform'):\n",
    "    \"\"\"Generate random predictions\"\"\"\n",
    "    if strategy == 'uniform':\n",
    "        # Predict 0.5 for everything (coin flip)\n",
    "        return pd.DataFrame(0.5, index=true_df.index, columns=go_terms)\n",
    "    elif strategy == 'proportional':\n",
    "        # Predict based on training frequency\n",
    "        train_freq = embeddings_dict_full['train'][go_terms].mean()\n",
    "        preds = np.tile(train_freq.values, (len(true_df), 1))\n",
    "        return pd.DataFrame(preds, index=true_df.index, columns=go_terms)\n",
    "\n",
    "# Generate baselines\n",
    "uniform_preds = make_random_predictions(valid_true_df, go_terms, 'uniform')\n",
    "proportional_preds = make_random_predictions(valid_true_df, go_terms, 'proportional')\n",
    "\n",
    "# Compute metrics for each method\n",
    "methods = {\n",
    "    'Uniform Random': uniform_preds,\n",
    "    'Proportional Random': proportional_preds,\n",
    "    'ESM-2 Model (batch_norm)': valid_pred_df\n",
    "}\n",
    "\n",
    "method_scores = []\n",
    "for method_name, preds in methods.items():\n",
    "    scores = []\n",
    "    for i, go_term in enumerate(go_terms):\n",
    "        y_true = valid_true_df.iloc[:, i].values\n",
    "        y_pred = preds.iloc[:, i].values\n",
    "        \n",
    "        if len(np.unique(y_true)) == 2:\n",
    "            try:\n",
    "                pr_auc = average_precision_score(y_true, y_pred)\n",
    "                scores.append(pr_auc)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    method_scores.append({\n",
    "        'Method': method_name,\n",
    "        'Mean PR-AUC': np.mean(scores),\n",
    "        'Median PR-AUC': np.median(scores)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(method_scores)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "bars = ax.bar(x_pos, comparison_df['Mean PR-AUC'], color=['gray', 'lightblue', 'darkblue'])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(comparison_df['Method'], rotation=15, ha='right')\n",
    "ax.set_ylabel('Mean PR-AUC', fontsize=12)\n",
    "ax.set_title('Model Performance vs Random Baselines', fontsize=14)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBaseline Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 5: Top 20 Best Performing Functions\n",
    "# =============================================================================\n",
    "\n",
    "# Get top 20 by model performance\n",
    "top_20 = performance_df.nlargest(20, 'pr_auc')\n",
    "\n",
    "# Prepare for plotting\n",
    "plot_data = top_20[['description', 'pr_auc']].reset_index()\n",
    "plot_data['description_short'] = plot_data['description'].str[:40]  # Truncate long names\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.barh(range(len(plot_data)), plot_data['pr_auc'], color='steelblue')\n",
    "ax.set_yticks(range(len(plot_data)))\n",
    "ax.set_yticklabels(plot_data['description_short'], fontsize=10)\n",
    "ax.set_xlabel('PR-AUC (Validation)', fontsize=12)\n",
    "ax.set_title(\"Top 20 Best Predicted Protein Functions\", fontsize=14)\n",
    "ax.invert_yaxis()\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.3, label='Random baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThese functions have strong sequence signatures that ESM-2 captures well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "Functions with conserved sequence motifs (kinases, receptors, DNA-binding domains) achieve PR-AUC >0.85. ESM-2 learned these evolutionary signatures without explicit structural supervision.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 6: Training History (Best Model)\n",
    "# =============================================================================\n",
    "\n",
    "# Re-train best model with history tracking\n",
    "print(\"Re-training best model (batch_norm) with full history tracking...\")\n",
    "\n",
    "best_config = configs['batch_norm']\n",
    "model_final = ProteinClassifier(\n",
    "    input_dim=X_train_full.shape[1],\n",
    "    hidden_dims=best_config[\"architecture\"],\n",
    "    num_classes=len(go_terms),\n",
    "    dropout=best_config[\"dropout\"],\n",
    "    use_batch_norm=best_config[\"batch_norm\"]\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_final.parameters(), lr=best_config[\"lr\"])\n",
    "\n",
    "# Training loop with full history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Training\n",
    "    model_final.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_outputs = model_final(X_train_t)\n",
    "    train_loss = criterion(train_outputs, y_train_t)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_acc = ((train_outputs > 0.5) == y_train_t).float().mean()\n",
    "    \n",
    "    # Validation\n",
    "    model_final.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model_final(X_valid_t)\n",
    "        val_loss = criterion(val_outputs, y_valid_t)\n",
    "        val_acc = ((val_outputs > 0.5) == y_valid_t).float().mean()\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss.item())\n",
    "    history['val_loss'].append(val_loss.item())\n",
    "    history['train_acc'].append(train_acc.item())\n",
    "    history['val_acc'].append(val_acc.item())\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_epoch = epoch\n",
    "        best_state = model_final.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 5:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "model_final.load_state_dict(best_state)\n",
    "print(f\"Best model from epoch {best_epoch}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Binary Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Analysis:\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f} (epoch {best_epoch})\")\n",
    "print(f\"Gap (train-val): {abs(history['train_loss'][-1] - history['val_loss'][-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This project successfully predicted protein molecular functions from amino acid sequences using ESM-2 embeddings and a multi-label neural network classifier. Training on 5,222 human proteins across 202 GO terms, the batch normalization model achieved 8x better performance than random baselines (mean PR-AUC 0.123 vs 0.016), with exceptional accuracy for functions with conserved sequence motifs like GPCRs (PR-AUC 0.996) and kinases (PR-AUC 0.892). Performance strongly correlates with training data availability, demonstrating that transfer learning from protein language models enables scalable functional annotation without requiring homology information or experimental characterization. This approach provides an interpretable, computationally efficient pipeline for proteome-wide functional prediction that can guide experimental prioritization and accelerate annotation of newly sequenced genomes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "all,-execution",
   "formats": "ipynb,md:myst",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,jupytext,language_info",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
